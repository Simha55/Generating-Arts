{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4LXniBxQtE7"
      },
      "source": [
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "class vgg_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(vgg_model, self).__init__()\n",
        "        self.vgg = models.vgg19(pretrained=True).features[:29]\n",
        "\n",
        "        self.chosen_layers = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for layer_num, layer in enumerate(self.vgg):\n",
        "            x = layer(x)\n",
        "\n",
        "            if str(layer_num) in self.chosen_layers:\n",
        "                features.append(x)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "def image(name):\n",
        "    img = Image.open(name)\n",
        "    img = loader(img).unsqueeze(0)\n",
        "    return img.to(device)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "size = 256\n",
        "loader = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "o_img = image(\"img.jpg\")\n",
        "s_img = image(\"img2.jpg\")\n",
        "gen = original_img.clone().requires_grad_(True)\n",
        "model = vgg_model().to(device).eval()\n",
        "\n",
        "steps = 6000\n",
        "learning_rate = 0.001\n",
        "alpha = 1\n",
        "beta = 0.01\n",
        "optim = optim.Adam([generated], lr=learning_rate)\n",
        "\n",
        "for step in range(steps):\n",
        "    generated_features = model(gen)\n",
        "    original_img_features = model(o_img)\n",
        "    style_features = model(s_img)\n",
        "    s_loss = o_loss = 0\n",
        "    for gen_feature, orig_feature, style_feature in zip(\n",
        "        generated_features, original_img_features, style_features\n",
        "    ):\n",
        "        batch_size, channel, height, width = gen_feature.shape\n",
        "        original_loss += torch.mean((gen_feature - orig_feature) ** 2)\n",
        "        G = gen_feature.view(channel, height * width).mm(\n",
        "            gen_feature.view(channel, height * width).t()\n",
        "        )\n",
        "        A = style_feature.view(channel, height * width).mm(\n",
        "            style_feature.view(channel, height * width).t()\n",
        "        )\n",
        "        style_loss += torch.mean((G - A) ** 2)\n",
        "\n",
        "    total_loss = alpha * original_loss + beta * style_loss\n",
        "    optim.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(total_loss)\n",
        "        save_image(gen, \"generated.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}